# object_tracker.py

import cv2
import torch
import time
import socket
import json
import numpy as np
from pathlib import Path
from strongsort.strong_sort import StrongSORT
import threading
import math

# ===== ì„¤ì • =====
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
YOLO_MODEL_NAME = 'yolov5m'
REID_WEIGHT_PATH = Path('./osnet_x1_0_msmt17.pt')
IMAGE_LISTEN_PORT = 7003
ROS_BRIDGE_IP = '127.0.0.1'
ROS_BRIDGE_PORT = 7008
CMD_LISTEN_PORT = 7009
REID_THRESHOLD = 0.4
FEATURE_UPDATE_ALPHA = 0.1

# ===== ëª¨ë¸ ë¡œë“œ =====
print("ğŸ¤– ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
yolo_model = torch.hub.load('ultralytics/yolov5', YOLO_MODEL_NAME, pretrained=True).to(DEVICE)
tracker = StrongSORT(
    model_weights=REID_WEIGHT_PATH, device=DEVICE, fp16=False,
    max_age=200, max_dist=0.3, max_iou_distance=0.7, n_init=3
)
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

# ===== ì†Œì¼“ ì„¤ì • =====
image_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
image_sock.bind(('0.0.0.0', IMAGE_LISTEN_PORT))
status_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
cmd_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
cmd_sock.bind(('0.0.0.0', CMD_LISTEN_PORT))

# ===== ìƒíƒœ ë³€ìˆ˜ =====
target_id = None
target_lost_time = None
target_mean_feature = None
find_center_target_flag = False # ì¤‘ì•™ íƒ€ê²Ÿ ì°¾ê¸° ëª…ë ¹ í”Œë˜ê·¸
state_lock = threading.Lock()

def command_listener():
    """ROS2 ë…¸ë“œë¡œë¶€í„° ëª…ë ¹ì„ ìˆ˜ì‹ í•˜ëŠ” ìŠ¤ë ˆë“œ"""
    global target_id, target_mean_feature, target_lost_time, find_center_target_flag
    print(f"ğŸ‘‚ ROS2 ëª…ë ¹ ìˆ˜ì‹  ëŒ€ê¸° ì‹œì‘ (í¬íŠ¸: {CMD_LISTEN_PORT})")
    while True:
        try:
            data, _ = cmd_sock.recvfrom(1024)
            command = json.loads(data.decode())
            
            with state_lock:
                if command.get('command') == 'activate_and_find_center':
                    print("ğŸ¯ ëª…ë ¹ ìˆ˜ì‹ : ì¤‘ì•™ íƒ€ê²Ÿ ì°¾ê¸° í™œì„±í™”")
                    # ê¸°ì¡´ íƒ€ê²Ÿ ì •ë³´ ì´ˆê¸°í™” ë° íƒ€ê²Ÿ ì°¾ê¸° í”Œë˜ê·¸ ì„¤ì •
                    target_id, target_mean_feature, target_lost_time = None, None, None
                    find_center_target_flag = True 
                elif command.get('command') == 'clear_target':
                    print("ğŸ—‘ï¸ ëª…ë ¹ ìˆ˜ì‹ : íƒ€ê²Ÿ í•´ì œ")
                    target_id, target_mean_feature, target_lost_time = None, None, None
                    find_center_target_flag = False # íƒ€ê²Ÿ ì°¾ê¸° ì¤‘ì§€
        except Exception as e:
            print(f"â— ëª…ë ¹ ìˆ˜ì‹  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

threading.Thread(target=command_listener, daemon=True).start()

print(f"ğŸš€ ì¶”ì  ì‹œìŠ¤í…œ ì‹œì‘. UDP í¬íŠ¸ {IMAGE_LISTEN_PORT}ì—ì„œ ì´ë¯¸ì§€ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")

try:
    while True:
        # 1. ì´ë¯¸ì§€ ìˆ˜ì‹  ë° ë””ì½”ë”©
        data, _ = image_sock.recvfrom(65536)
        separator_pos = data.find(b'|')
        if separator_pos == -1: continue
        jpeg_bytes = data[separator_pos+1:]
        np_arr = np.frombuffer(jpeg_bytes, np.uint8)
        frame = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
        if frame is None: continue

        # 2. ê°ì²´ íƒì§€ ë° ì¶”ì 
        results = yolo_model(frame)
        detections = results.xyxy[0]
        person_detections = detections[detections[:, 5] == 0]
        tracked_outputs = tracker.update(person_detections.cpu(), frame) if len(person_detections) > 0 else []
        if len(person_detections) == 0: tracker.increment_ages()

        current_tracks = {t.track_id: t for t in tracker.tracker.tracks}
        found_target = False
        lost_time = 0

        with state_lock:
            # 2.1 ì¤‘ì•™ íƒ€ê²Ÿ ì°¾ê¸° ë¡œì§ (í”Œë˜ê·¸ê°€ Trueì¼ ë•Œë§Œ ì‹¤í–‰)
            if find_center_target_flag and len(tracked_outputs) > 0:
                frame_center_x, frame_center_y = frame.shape[1] // 2, frame.shape[0] // 2
                min_dist = float('inf')
                center_target_id = None

                for output in tracked_outputs:
                    x1, y1, x2, y2, track_id_out = map(int, output[:5])
                    box_center_x, box_center_y = (x1 + x2) / 2, (y1 + y2) / 2
                    dist = math.sqrt((box_center_x - frame_center_x)**2 + (box_center_y - frame_center_y)**2)
                    
                    if dist < min_dist:
                        min_dist = dist
                        center_target_id = track_id_out
                
                if center_target_id is not None:
                    target_id = center_target_id
                    print(f"âœ… ì¤‘ì•™ íƒ€ê²Ÿ ê²°ì •: ID {target_id}")
                
                find_center_target_flag = False # í”Œë˜ê·¸ë¥¼ ë‹¤ì‹œ êº¼ì„œ ë‹¤ìŒ í”„ë ˆì„ì—ì„œ ë°˜ë³µ ì‹¤í–‰ ë°©ì§€

            # 2.2 ê¸°ì¡´ íƒ€ê²Ÿ ì¶”ì  ë° ì¬ì‹ë³„ ë¡œì§
            if target_id is not None and target_id in current_tracks:
                # ... (ì´í•˜ ê¸°ì¡´ ì¶”ì  ë¡œì§ì€ ë™ì¼)
                found_target = True
                target_lost_time = None
                current_feature = current_tracks[target_id].features[-1]
                if target_mean_feature is None:
                    target_mean_feature = current_feature
                else:
                    target_mean_feature = (1 - FEATURE_UPDATE_ALPHA) * target_mean_feature + FEATURE_UPDATE_ALPHA * current_feature
                    target_mean_feature /= np.linalg.norm(target_mean_feature)
            
            if not found_target and target_id is not None:
                if target_lost_time is None: target_lost_time = time.time()
                lost_time = time.time() - target_lost_time

                if target_mean_feature is not None and len(current_tracks) > 0:
                    best_match_id, min_dist = -1, float('inf')
                    for track_id, track in current_tracks.items():
                        if track.is_confirmed() and track.features:
                            dist = 1 - np.dot(track.features[-1], target_mean_feature)
                            if dist < min_dist: min_dist, best_match_id = dist, track_id
                    
                    if min_dist < REID_THRESHOLD:
                        print(f"ğŸ”„ íƒ€ê²Ÿ ì¬ì‹ë³„ ì„±ê³µ! ID {target_id} -> {best_match_id} (ê±°ë¦¬: {min_dist:.3f})")
                        target_id, target_lost_time, lost_time = best_match_id, None, 0
        
        # 3. ìƒíƒœ ì „ì†¡ ë° ì‹œê°í™”
        message = {'timestamp': time.time(), 'lost_time': lost_time}
        status_sock.sendto(json.dumps(message).encode(), (ROS_BRIDGE_IP, ROS_BRIDGE_PORT))

        for output in tracked_outputs:
            x1, y1, x2, y2, track_id_out = map(int, output[:5])
            color, label = ((255, 0, 0), f"ID {track_id_out}")
            if track_id_out == target_id:
                color, label = ((0, 255, 0), f"TARGET: {track_id_out}")
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        if lost_time > 0:
            cv2.putText(frame, f"TARGET LOST: {lost_time:.1f}s", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        cv2.imshow("Object Tracker (UDP Input)", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

except KeyboardInterrupt:
    print("\n[ì¢…ë£Œ] ì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
finally:
    print("ì†Œì¼“ì„ ë‹«ê³  í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤...")
    image_sock.close()
    status_sock.close()
    cmd_sock.close()
    cv2.destroyAllWindows()